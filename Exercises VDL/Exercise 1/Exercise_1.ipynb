{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/mindgarage/very-deep-learning-wise2324/blob/main/exercises/Exercise_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MHvuWIUHdIpt"
   },
   "source": [
    "# Programming Exercise 1: Introduction to Deep Learning with PyTorch\n",
    "\n",
    "## Very Deep Learning (VDL) - Winter Semester 2023/24\n",
    "\n",
    "---\n",
    "\n",
    "### Group Details:\n",
    "\n",
    "- **Group Name:** \\[VDL_Group_11\\]\n",
    "\n",
    "### Members:\n",
    "\n",
    "- \\[Chirag Singh], \\[428971]\n",
    "- \\[Maithilee Vaidya], \\[428837\\]\n",
    "- \\[Prachi Tejwani\\], \\[429016\\]\n",
    "- \\[Prathamesh Pawar\\], \\[428966\\]\n",
    "- \\[Sankalp Dhupar\\], \\[429026\\]\n",
    "- \\[Vibhor Chauhan\\], \\[428986\\]\n",
    "\n",
    "---\n",
    "\n",
    "**Instructions**: The tasks in this notebook are a part of Sheet 1. Look for `TODO` tags throughout the notebook and complete the sections with missing code. Once done, ensure all outputs are visible and correctly displayed. Save your notebook and submit the `.ipynb` file together with the exercise sheet PDF in a single ZIP file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QlLRHLb6gl_i"
   },
   "source": [
    "## Introduction:\n",
    "\n",
    "Welcome to the first programming exercise of the Very Deep Learning course. In this exercise, you will be introduced to PyTorch, one of the most widely used deep learning frameworks in academia and industry. With its dynamic computation graph and vast ecosystem, PyTorch provides an intuitive and versatile platform for building various deep learning models.\n",
    "\n",
    "The aim of this task is to familiarize you with the basics of [PyTorch](https://pytorch.org/) and [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/starter/introduction.html), guiding you in building, training, and evaluating a simple neural network model. You'll be working with the [Flowers102 dataset](https://pytorch.org/vision/stable/generated/torchvision.datasets.Flowers102.html), a collection of images representing 102 flower categories. The flowers chosen are commonly occuring in the United Kingdom. Each class consists of between 40 and 258 images. By the end of this exercise, you should have a foundational understanding of neural networks, how they are trained, and how to evaluate their performance.\n",
    "\n",
    "---\n",
    "#### Remember not to modify the number of epochs. All teams must train only for 10 epochs.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aGDCwSofQuPt"
   },
   "outputs": [],
   "source": [
    "# Install dependencies.\n",
    "# Note: You can execute bash commands inside Google Colab!\n",
    "!pip install pytorch-lightning # reduces boilerplate in vanilla PyTorch\n",
    "!pip install torchmetrics      # simplifies metric computation\n",
    "!pip install pandas            # for reading training logs from CSV\n",
    "# We also need `torch` and `torchvision`, but they come pre-installed inside Colab. If not uncomment the lines below:\n",
    "# !pip install torch\n",
    "# !pip install torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uLdtrS4zaeEs"
   },
   "source": [
    "## Step 1: Data Preparation\n",
    "\n",
    "The first step in any deep learning pipeline is data preparation. Here, we will download the ImageNet dataset and prepare it for training and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O2YVG6DPOyui"
   },
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "pl.seed_everything(42) # seed to make randomness deterministic\n",
    "\n",
    "from matplotlib.image import NonUniformImage\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import Flowers102\n",
    "\n",
    "\n",
    "# Define data transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,)),\n",
    "])\n",
    "\n",
    "# Download and load the Flowers102 dataset\n",
    "train_dataset = Flowers102(root='./data', split='train', download=True, transform=transform)\n",
    "test_dataset = Flowers102(root='./data', split='test', download=True, transform=transform)\n",
    "\n",
    "# TODO: Split train dataset into train and val sets using an 80/20 ratio\n",
    "# HINT: Use torch.utils.data.random_split\n",
    "train_dataset, val_dataset = None, None\n",
    "\n",
    "# TODO: Create train, val and test DataLoaders\n",
    "# HINT: Set `shuffle=True` for the train set.\n",
    "batch_size = 64\n",
    "num_workers = 2\n",
    "train_loader = None\n",
    "val_loader = None\n",
    "test_loader = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UbgrhUjDQ7Xh"
   },
   "source": [
    "## Step 2: Model Definition\n",
    "\n",
    "For this tutorial, we'll use a pre-trained ResNet-18 model, which is a popular model for image classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sZD2NGz2Ak6w"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "\n",
    "\n",
    "class Flowers102Classifier(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(Flowers102Classifier, self).__init__()\n",
    "\n",
    "        # Create a pre-trained ResNet-18 model from `torchvision` instead of writing\n",
    "        # our own model. This model was trained on the ImageNet dataset, which has\n",
    "        # RGB images with 1000 different classes.\n",
    "        self.model = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "        # HINT: Use print(self.model) to see the architecture\n",
    "\n",
    "        # TODO: Modify the architecture as you see fit :)\n",
    "\n",
    "        # The Flowers102 dataset 102 classes.\n",
    "        # TODO: Modify the last layer to fit the number of classes in Flowers102.\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GcAfrn2falkK"
   },
   "source": [
    "## Step 3: Training with PyTorch Lightning\n",
    "\n",
    "PyTorch Lightning simplifies the training loop. Let's create a Lightning module for our classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xOjlOyjcCezX"
   },
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "import torch.optim as optim\n",
    "from torchmetrics import Accuracy\n",
    "\n",
    "\n",
    "class ClassificationModule(pl.LightningModule):\n",
    "    \"\"\" A PyTorch Lightning module for contains both the network and the\n",
    "    training logic, unlike simple PyTorch code we saw in the first tutorial. \"\"\"\n",
    "    def __init__(self, learning_rate=0.001, num_classes=10):\n",
    "        super(ClassificationModule, self).__init__()\n",
    "        self.save_hyperparameters() # allows access to constructor args with self.hparams.*\n",
    "\n",
    "        self.model = Flowers102Classifier(num_classes=num_classes)\n",
    "\n",
    "        # TODO: Define a loss function\n",
    "        # HINT: This is a classification task with multiple classes.\n",
    "        self.loss_fn = None\n",
    "\n",
    "        # TODO: Create an appropriate metric from torchmetrics\n",
    "        self.metric = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        images, labels = batch\n",
    "        outputs = self(images) # Forward pass\n",
    "        loss = self.loss_fn(outputs, labels)\n",
    "        self.log('train_loss', loss)\n",
    "\n",
    "        # Note: We do not need to manually call loss.backward() or optim.step()\n",
    "        # when using PyTorch Lightning\n",
    "        return loss\n",
    "\n",
    "    def on_validation_epoch_start(self):\n",
    "        self.metric.reset()\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        images, labels = batch\n",
    "        outputs = self(images)\n",
    "        loss = self.loss_fn(outputs, labels)\n",
    "        self.log('val_loss', loss, prog_bar=True)\n",
    "\n",
    "        # Update accuracy for current batch\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        self.metric.update(preds, labels)\n",
    "        return loss\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        avg_accuracy = self.metric.compute()\n",
    "        self.log('val_accuracy', avg_accuracy, prog_bar=True)\n",
    "\n",
    "    def on_test_epoch_start(self):\n",
    "        self.metric.reset()\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        images, labels = batch\n",
    "        outputs = self(images)\n",
    "\n",
    "        # Note: We do not need to calculate loss when evaluating\n",
    "        # on the test dataset, only the performance metric!\n",
    "\n",
    "        # Update accuracy for current batch\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        self.metric.update(preds, labels)\n",
    "        return {\"test_accuracy\": self.metric}\n",
    "\n",
    "    def on_test_epoch_end(self):\n",
    "        avg_accuracy = self.metric.compute()\n",
    "        self.log('test_accuracy', avg_accuracy, prog_bar=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=self.hparams.learning_rate)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ms333UK1RKyb"
   },
   "source": [
    "## Step 4: Training the Model\n",
    "\n",
    "Now that our Lightning module is defined, we can easily train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EuBvOWmGDHOq"
   },
   "outputs": [],
   "source": [
    "# Initialize the classifier\n",
    "classifier = ClassificationModule()\n",
    "\n",
    "# Create a logger\n",
    "# HINT: Lightning has many different kinds of loggers, such\n",
    "# as Tensorboard, WandB, Comet, etc.\n",
    "# https://lightning.ai/docs/pytorch/stable/api_references.html#loggers\n",
    "logger = pl.loggers.CSVLogger('./logs')\n",
    "\n",
    "# Initialize a trainer\n",
    "trainer = pl.Trainer(\n",
    "    deterministic=True,\n",
    "    accelerator='gpu' if torch.cuda.is_available() else 'cpu',\n",
    "    logger=logger,\n",
    "    max_epochs=10, # NOTE: DO NOT MODIFY THE NUMBER OF EPOCHS. YOU MUST ONLY TRAIN FOR 10.\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.fit(classifier, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lGyau0mOaP2m"
   },
   "source": [
    "## Step 5: Testing the Model\n",
    "\n",
    "Let's write a simple loop to test the model's predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F9CppCcqDLtB"
   },
   "outputs": [],
   "source": [
    "# TODO: Test the network performance with test_loader\n",
    "acc = None\n",
    "print(f\"Accuracy: {(acc[0]['test_accuracy'] * 100):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M_uV1-nWq3W0"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "log_file = './logs/lightning_logs/version_0/metrics.csv'\n",
    "logs = pd.read_csv(log_file)\n",
    "print(logs.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_oUjtgnnrLa3"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_metrics(df):\n",
    "    df_train = df[['epoch', 'step', 'train_loss']].dropna()\n",
    "    df_train = df_train.groupby('epoch').apply(lambda x: x.loc[x['step'].idxmax()])[['epoch', 'step', 'train_loss']]\n",
    "    df_val = df[['epoch', 'step', 'val_loss', 'val_acc']].dropna()\n",
    "    df_test = df[['epoch', 'step', 'test_accuracy']].dropna()\n",
    "\n",
    "    # Set up the figure and axes\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "    # Plot train_loss and val_loss on the first subplot\n",
    "    axs[0].plot(df_train['epoch'], df_train['train_loss'], label='Train Loss', color='blue')\n",
    "    axs[0].plot(df_val['epoch'], df_val['val_loss'], label='Validation Loss', color='red', linestyle='dashed')\n",
    "    axs[0].set_title('Train Loss vs Validation Loss')\n",
    "    axs[0].set_xlabel('Step')\n",
    "    axs[0].set_ylabel('Loss')\n",
    "    axs[0].legend()\n",
    "\n",
    "    # Plot val_acc and test_accuracy on the second subplot\n",
    "    axs[1].plot(df_val['epoch'], df_val['val_acc'], label='Validation Accuracy', color='green')\n",
    "    axs[1].plot(df_test['epoch'], df_test['test_accuracy'], label='Test Accuracy', color='orange', linestyle='dashed')\n",
    "    axs[1].set_title('Validation Accuracy vs Test Accuracy')\n",
    "    axs[1].set_xlabel('Step')\n",
    "    axs[1].set_ylabel('Accuracy')\n",
    "    axs[1].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_metrics(logs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JQ56jVQbsyO9"
   },
   "outputs": [],
   "source": [
    "# !rm -rf ./logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n-NR96UtFSkB"
   },
   "source": [
    "## Homework/Exercise\n",
    "\n",
    "1. Complete missing code and run the notebook.\n",
    "2. Experiment with different network architectures and hyperparameters to try and improve the classification accuracy.\n",
    "\n",
    "You are allowed to change:\n",
    "  - network architecture, including using other torchvision models\n",
    "  - optimizer\n",
    "  - learning rate\n",
    "  - batch size\n",
    "  - loss function\n",
    "\n",
    "You can also experiment with [learning rate scheduling](https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate). In [Lightning](https://lightning.ai/docs/pytorch/stable/api/lightning.pytorch.core.LightningModule.html#lightning.pytorch.core.LightningModule), you will add the scheduler in the `configure_optimizer` function by returning something like `return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler}`.\n",
    "\n",
    "However, changing the number of training epochs is **not allowed**. Train your network for **10 epochs**!\n",
    "\n",
    "The group with the best results gets a small prize :)\n",
    "\n",
    "Good luck!"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
